# Project Collection

This repository contains three distinct projects developed as part of our class assignments. Each project explores a different aspect of modern machine learning—from model compression via knowledge distillation, to theoretical investigations of neural network training dynamics, and finally, to the estimation of optical flow using physics-informed neural networks (PINNs).

---

## File 1: Knowledge Distillation using Teacher and Student ResNets

In this project, we demonstrate a version of knowledge distillation called **feature distillation**. The idea is based on the paper [A Simple and Generic Framework for Feature Distillation via Channel-wise Transformation](https://arxiv.org/abs/2303.13212) by Ziwei Liu, Yongtao Wang, and Xiaojie Chu.

### Overview

- **Teacher Model:**  
  We use a large pretrained ResNet (e.g., **ResNet-152**) as our teacher. The teacher network produces:
  - **Final logits:** The output predictions over the classes.
  - **Intermediate features:** A pooled feature vector extracted from the output of its **second residual block**.

  Mathematically, for an input image \( x \), the teacher produces:
  
  $$
  \mathbf{T}_{\text{logits}} = f_{\text{teacher}}(x)
  $$
  
  and an intermediate representation:
  
  $$
  \mathbf{T}_{\text{feat}} = g_{\text{teacher}}(x).
  $$

- **Student Model:**  
  The student is a smaller ResNet (e.g., **ResNet-18**) that is modified for knowledge distillation by attaching an additional branch to its second residual block. This branch consists of two linear layers with a ReLU activation in between. The student produces:
  
  1. **Main logits:**
     
     $$
     \mathbf{S}_{\text{logits}} = f_{\text{student}}(x)
     $$
  
  2. **Auxiliary output:**
     
     $$
     \mathbf{S}_{\text{aux}} = h_{\text{student}}(g_{\text{student}}(x))
     $$
  
  The overall output of the student is the concatenation:
  
  $$
  \mathbf{S}_{\text{output}} = \left[ \mathbf{S}_{\text{logits}},\; \mathbf{S}_{\text{aux}} \right].
  $$

- **Distillation Loss:**  
  The student is trained using a combined Mean Squared Error (MSE) loss to mimic the teacher:
  
  1. **Logit Loss:**
  
     $$
     L_{\text{logits}} = \left\| \mathbf{S}_{\text{logits}} - \mathbf{T}_{\text{logits}} \right\|^2
     $$
  
  2. **Feature Loss:**
  
     $$
     L_{\text{feat}} = \left\| \mathbf{S}_{\text{aux}} - \mathbf{T}_{\text{feat}} \right\|^2
     $$
  
  The total loss is:
  
  $$
  L_{\text{total}} = L_{\text{logits}} + L_{\text{feat}}.
  $$

- **Data Generation using BigGAN:**  
  BigGAN is used to generate synthetic images, ensuring that the training process sees a diverse set of inputs. This helps the student network learn robust representations from the teacher.

### Summary

- **Teacher:** A large ResNet (e.g., ResNet-152) provides both final predictions and intermediate features.
- **Student:** A smaller ResNet (e.g., ResNet-18) augmented with an extra branch that outputs auxiliary features.
- **Loss Function:** Combined MSE loss on both the logits and the features.
- **Data:** Synthetic images generated by BigGAN.

---

## File 2: Mean Field View of the Landscape of Two-Layer Neural Networks

This project reproduces some computations from the paper [A Mean Field View of the Landscape of Two-Layer Neural Networks](https://arxiv.org/abs/1804.06561) by Song Mei, Andrea Montanari, and Phan-Minh Nguyen.

### Overview

The goal of this project is to study the evolution of the distribution of neural network weights in the mean-field limit when training with vanilla SGD. We consider a classification task with labels \(\{-1, +1\}\) and learn the mapping \((x_i, y_i)\).

### Model

The two-layer neural network studied is defined as:
  
$$
\hat{y}(x) := \frac{1}{N} \sum_{i=1}^N a_i \, \sigma(w_i \cdot x + b_i)
$$

where:
- \( x \in \mathbb{R}^d \) is the input,
- \( w_i \in \mathbb{R}^{d \times D} \) and \( b_i \in \mathbb{R}^D \) are parameters,
- \(\sigma: \mathbb{R}^D \to \mathbb{R}\) is the activation function, and
- \(\theta_i = (a_i, w_i, b_i)\) represents the collection of parameters for neuron \( i \).

The model parameters are optimized by minimizing the population risk:
  
$$
R_N(\theta) = \mathbb{E}\left[(y - \hat{y}(x,\theta))^2\right],
$$

which is essentially the Mean Squared Error (MSE) loss.

### Mean-Field Formulation

The risk can be expressed as:
  
$$
R_N(\theta) = R_{\text{base}} + \frac{2}{N} \sum_{i=1}^{N} V(\theta_i) + \frac{1}{N^2} \sum_{i,j=1}^{N} U(\theta_i, \theta_j),
$$

with:
- \( V(\theta) = -\mathbb{E}\{ y \, \sigma_{*}(x; \theta) \} \),
- \( U(\theta_1, \theta_2) = \mathbb{E}\{ \sigma_{*}(x; \theta_1) \, \sigma_{*}(x; \theta_2) \} \),
- \( R_{\text{base}} = \mathbb{E}\{y^2\} \).

Since \( R_N(\theta) \) depends on the parameters only through their empirical distribution,
  
$$
\hat{\rho}^{(N)} = \frac{1}{N} \sum_{i=1}^{N} \delta_{\theta_i},
$$

we define the risk functional for a distribution \(\rho\) as:
  
$$
R(\rho) = R_{\text{base}} + 2 \int V(\theta) \, \rho(d\theta) + \int U(\theta_1, \theta_2) \, \rho(d\theta_1) \rho(d\theta_2).
$$

Under mild assumptions, it can be shown that:
  
$$
\inf_{\theta} R_N(\theta) = \inf_{\rho} R(\rho) + O(1/N).
$$

### SGD and Continuum Dynamics

We model SGD as a continuous-time process:
  
$$
\theta(t+\Delta t) = \theta(t) + \xi(t) \nabla_\theta J(y(x), \hat{y}(x,\theta)),
$$

where \(\xi(t)\) is the (possibly time-varying) step size.

The empirical distribution of parameters after \(k\) SGD steps is:
  
$$
\hat{\rho}_k^{(N)} = \frac{1}{N} \sum_{i=1}^{N} \delta_{\theta_k^i}.
$$

As \(N \to \infty\) and the step size becomes small, this distribution converges (in a weak sense) to a continuum distribution \(\rho_t\) which satisfies the following **distributional dynamics (DD)** PDE:
  
$$
\partial_t \rho_t = 2 \xi(t) \nabla_{\theta} \cdot \left( \rho_t \nabla_{\theta} \Psi(\theta; \rho_t) \right),
$$

with

$$
\Psi(\theta; \rho) \equiv V(\theta) + \int U(\theta, \theta') \, \rho(d\theta').
$$

In our experiments, we compute the model parameter distribution for a binary classification task (distinguishing between two Gaussians) and show that it is well approximated by the solution of the DD PDE.

---

## File 3: Optical Flow Estimation with a Physics-Informed Neural Network (PINN)

This project focuses on estimating optical flow using a Physics-Informed Neural Network (PINN). The optical flow problem is based on the brightness constancy assumption, which states that the brightness of a point remains constant between consecutive frames.

### Optical Flow Fundamentals

The brightness constancy assumption can be written as an inline equation:
  
$$
I(x,y,t) = I(x+u,\, y+v,\, t+\Delta t)
$$

where \( u \) and \( v \) are the horizontal and vertical components of the optical flow. By linearizing the above assumption using a first-order Taylor expansion, we obtain the linearized brightness constancy constraint:
  
$$
I_x \, u + I_y \, v + I_t = 0,
$$

where
- \( I_x \) and \( I_y \) are the spatial derivatives of the image intensity,
- \( I_t \) is the temporal derivative, and
- \( u \) and \( v \) are the optical flow components.

### Horn–Schunck Method

The Horn–Schunck method formulates the estimation of optical flow as the minimization of an energy functional that combines the brightness constancy term with a smoothness term:
  
$$
E(u,v) = \iint \left[ \left( I_x \, u + I_y \, v + I_t \right)^2 + \alpha^2 \left( \|\nabla u\|^2 + \|\nabla v\|^2 \right) \right] \, dx \, dy,
$$

where \(\alpha\) is a regularization parameter that controls the smoothness of the flow field. Minimizing this energy leads to the following Euler–Lagrange equations:
  
$$
I_x \left( I_x \, u + I_y \, v + I_t \right) - \alpha^2 \nabla^2 u = 0,
$$

$$
I_y \left( I_x \, u + I_y \, v + I_t \right) - \alpha^2 \nabla^2 v = 0.
$$

### PINN Approach for Optical Flow

In our implementation, we solve the optical flow PDE using a PINN, which embeds these physical constraints directly into the loss function during training. The network takes normalized pixel coordinates as input and outputs the optical flow components \([u,v]\) for the entire image.

#### Training and Testing Strategy

- **Training on 2/3 of the Video Frames:**  
  We use consecutive frame pairs from the first two-thirds of the video for training. For each pair, the spatial derivatives \(I_x\) and \(I_y\) and the temporal derivative \(I_t\) are computed. The network is trained on a random subset of collocation points using a combined loss that penalizes the residual of the optical flow PDE.

- **Testing on the Final 1/3 of the Frames:**  
  After training, we evaluate the network on selected frame pairs from the final one-third of the video. We compare the optical flow computed by the classical Horn–Schunck method with the flow predicted by the PINN. The differences are visualized, illustrating how well the PINN generalizes to unseen data.

### Summary

- **Physics-Informed Approach:** The PINN is trained to satisfy the brightness constancy and smoothness constraints inherent in the Horn–Schunck formulation.
- **Evaluation:** We demonstrate the network’s performance by showing sample images of the computed optical flow, the PINN-predicted flow, and the difference between the two.

---

## Conclusion

This repository showcases three projects that explore different aspects of modern machine learning and computational modeling:
1. **Knowledge Distillation:** Transferring knowledge from a large ResNet to a smaller one using feature distillation.
2. **Mean Field Analysis:** Reproducing theoretical computations that describe the landscape of two-layer neural networks in the mean-field limit.
3. **Optical Flow Estimation:** Solving the optical flow PDE with a PINN and validating its performance on video data.

Each project is self-contained and demonstrates innovative approaches to challenging problems in deep learning and computational modeling. Contributions, feedback, and suggestions are welcome!

